from __future__ import annotations"""calibration.py â€“ deterministic and Bayesian calibration of a Surrogate."""import loggingimport mathfrom dataclasses import dataclassfrom typing import Mapping, Sequence, Unionimport emceeimport numpy as npfrom scipy.optimize import minimize, OptimizeResultfrom chromasurr.surrogate import Surrogate_logger = logging.getLogger(__name__)__all__ = ["CalibrationResult", "calibrate_surrogate", "bayesian_calibration"]EPS: float = 1e-12@dataclass(slots=True)class CalibrationResult:    """Container for calibration outputs."""    x_opt: np.ndarray    success: bool    fun: float    extra: dict[str, object]def _sigma_total(sigma_pred: float, sigma_obs: float) -> float:    """Combine observation and emulator uncertainties."""    return max(math.hypot(sigma_pred, sigma_obs), EPS)def _residual_sum_squares(    params: np.ndarray,    surrogate: Surrogate,    y_obs: Mapping[str, float],    sigma_obs: Mapping[str, float],    metric_weights: Mapping[str, float],) -> float:    """Compute weighted residual sum-of-squares across metrics."""    ssq = 0.0    X = params.reshape(1, -1)    for m, y_true in y_obs.items():        y_pred = surrogate.predict(m, X).item()        sigma_pred = math.sqrt(surrogate.predict_var(m, X).item())        sigma = _sigma_total(sigma_pred, sigma_obs.get(m, 0.0))        w = metric_weights.get(m, 1.0)        ssq += w * ((y_pred - y_true) / sigma) ** 2    return ssqdef _log_posterior(    params: np.ndarray,    surrogate: Surrogate,    y_obs: Mapping[str, float],    sigma_obs: Mapping[str, float],    metric_weights: Mapping[str, float],) -> float:    """Evaluate Gaussian log-likelihood (including normalization constants)."""    lp = 0.0    X = params.reshape(1, -1)    for m, y_true in y_obs.items():        y_pred = surrogate.predict(m, X).item()        sigma_pred = math.sqrt(surrogate.predict_var(m, X).item())        sigma = _sigma_total(sigma_pred, sigma_obs.get(m, 0.0))        w = metric_weights.get(m, 1.0)        lp -= 0.5 * w * (            ((y_pred - y_true) / sigma) ** 2 + 2.0 * math.log(sigma) + math.log(2.0 * math.pi)        )    return lpdef calibrate_surrogate(    surrogate: Surrogate,    *,    y_obs: Union[Mapping[str, float], float],    metric: str | None = None,    sigma_obs: Mapping[str, float] | None = None,    metric_weights: Mapping[str, float] | None = None,    x0: np.ndarray | None = None,    bounds: Sequence[tuple[float, float]] | None = None,    method: str = "L-BFGS-B",    tol: float | None = 1e-8,    maxiter: int = 500,    disp: bool = False,) -> CalibrationResult:    """Perform point-estimate calibration via weighted least squares.    Handles both new mapping API and legacy single-metric API.    Parameters    ----------    surrogate : Surrogate        The trained surrogate model to calibrate.    y_obs : mapping or float        Observed metric values. If `metric` is provided, `y_obs` can be a single float;        otherwise it must be a mapping from metric names to observed values.    metric : str, optional        Name of the metric corresponding to `y_obs` when `y_obs` is a single float.    sigma_obs : mapping, optional        Observation uncertainties by metric.    metric_weights : mapping, optional        Weights for each metric in the loss function.    x0 : array-like, optional        Initial guess for parameters.    bounds : sequence of (low, high) pairs, optional        Bounds on parameters.    method : str, default "L-BFGS-B"        Optimization method passed to `scipy.optimize.minimize`.    tol : float, optional        Tolerance for optimization termination.    maxiter : int, default 500        Maximum number of iterations.    disp : bool, default False        If True, prints convergence messages.    Returns    -------    CalibrationResult        *x_opt* best-fit parameters, *fun* RSS value, *extra* holds OptimizeResult.    """    # normalize y_obs    if metric is not None:        # legacy single-metric API        if isinstance(y_obs, Mapping):            y_map = y_obs  # mapping provided directly        else:            y_map = {metric: float(y_obs)}    else:        if not isinstance(y_obs, Mapping):            raise ValueError("When `metric` is not provided, `y_obs` must be a mapping of metric names to values.")        y_map = y_obs  # type: ignore[assignment]    sigma_obs = sigma_obs or {}    metric_weights = metric_weights or {}    bounds = bounds or surrogate.problem["bounds"]  # type: ignore[index]    if x0 is None:        x0 = np.mean(np.asarray(bounds, dtype=float), axis=1)    obj = lambda p: _residual_sum_squares(p, surrogate, y_map, sigma_obs, metric_weights)    res: OptimizeResult = minimize(        obj,        x0=x0,        bounds=bounds,        method=method,        options={"maxiter": maxiter, "disp": disp},        tol=tol,    )    return CalibrationResult(        x_opt=res.x.copy(),        success=res.success,        fun=float(res.fun),        extra={"opt_result": res},    )def bayesian_calibration(    surrogate: Surrogate,    *,    y_obs: Mapping[str, float],    sigma_obs: Mapping[str, float] | None = None,    metric_weights: Mapping[str, float] | None = None,    n_walkers: int | None = None,    n_steps: int = 5000,    burn_in: int | None = None,    initial_radius: float = 1e-2,    seed: int | None = None,    progress: bool = True,) -> CalibrationResult:    """Bayesian calibration via emcee EnsembleSampler."""    sigma_obs = sigma_obs or {}    metric_weights = metric_weights or {}    ndim = surrogate.problem["num_vars"]    bounds = np.asarray(surrogate.problem["bounds"], dtype=float)    det = calibrate_surrogate(        surrogate,        y_obs=y_obs,        sigma_obs=sigma_obs,        metric_weights=metric_weights,    )    x0 = det.x_opt    rng = np.random.default_rng(seed)    n_w = n_walkers or max(4 * ndim, 20)    p0 = x0 + initial_radius * rng.standard_normal((n_w, ndim))    p0 = np.clip(p0, bounds[:, 0], bounds[:, 1])    def log_prob(theta: np.ndarray) -> float:        if np.any(theta < bounds[:, 0]) or np.any(theta > bounds[:, 1]):            return -np.inf        return _log_posterior(theta, surrogate, y_obs, sigma_obs, metric_weights)    sampler = emcee.EnsembleSampler(n_w, ndim, log_prob, seed=seed)    sampler.run_mcmc(p0, n_steps, progress=progress)    bi = burn_in or int(0.2 * n_steps)    chain = sampler.get_chain(discard=bi, flat=True)    lp = sampler.get_log_prob(discard=bi, flat=True)    idx = int(np.argmax(lp))    return CalibrationResult(        x_opt=chain[idx].copy(),        success=True,        fun=float(lp[idx]),        extra={"sampler": sampler, "chain": chain, "logp": lp},    )